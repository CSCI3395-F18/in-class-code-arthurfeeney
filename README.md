# Big Data Set Report - Arthur Feeney

Imagenet-12

This is a dataset of labeled images that was used for ILSVRC 2012 image classification competition. It has 1.2 million training images and 100,000 validation images. The training images are 138 GB. I currently have it installed on leviosa. As far as i am aware, classification is not as useful for inference tasks as regression is. So, on its own, there is nothing particularly interesting you can learn from it. However, there is a concept from neural networks called transfer learning. Transfer learning basically 'trains' a machine learning algorithm with a very large dataset, so the algorithm can be used with a small, similar dataset. I.e., I am almost certain the state-of-the-art for the CIFAR-10 dataset uses transfer learning with ImageNet. 

The main question (maybe not a question...) I have may be a little ambitious, but I think it would be more interesting than just finding a correlation between a couple variables. It deals more with methodology rather than inference. I saw that the Netflix dataset was on the syllabus, so we may cover this it in class, but I have been doing a lot of reading on Locaility Sensitive Hashing (LSH) for my research (I think it's technically a family of algorithms, but I just like to treat as one thing). Clearly, the foundation of the algorithm is hash tables! The most simple version, that probably will not work well, is to just construct a hash table with a function chosen uniformly from a locality-sensitive hash family. A hash family is locality sensitive if it has a high probability of
hashing similar points into the same bucket and distant points into different buckets. 

So, a table constructed with it can be used very efficiently for near neighbor search. If one has a point q and you want to find a near neighbor, you can hash q and only search through that bucket. If done well, there should be a high probability that a near neighbor is in the bucket (a single table with a single hash function will probably do quite poorly). I think it is/was actually common to use it for recommender systems, like Netflix, because you can do a neighbor search to find similar movies. Anyways, there is an improvement to LSH where the hash functions are trained. So, each tables hash function acts like a percpetron. I think it would be interesting to try transfer learning on LSH with updating hashing functions. The hash table would be constructed and the hash functions would be updated using ImageNet. Then it would just be used with some random dataset. Maybe the Netflix one? That would be kind of cool if it worked well. 

I expect LSH is built into spark. I had to implement my own great (very bad) version for my research because I wanted everything to be on the GPU and work with pytorch tensors, so I have not used any but I believe its in some python packages. I may just be able to fiddle with it some to try to get the hash functions to update. If it isn't in spark, LSH is surprisingly simple to implement and a perceptron is like insanely easy to make. I think the hardest part would just be putting it all together... And using ImageNet may not be viable because pandora may not be enough. I probably need to read the paper on the updating hash functions... Because I am confusing myself while writing this. A simple perceptron is just a linear regression, but when I looked at something about it, it sounded like they were treating the hash functions like a neural network... Another complication I am confused about is that pretty much every LSH function I have read about that computes an inner product requires the random one to be drawn from a normal distribution, but updating it would break that. 
